{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\freda\\miniconda3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from helper import (\n",
    "    start_time,\n",
    "    time_since,\n",
    "    ShakespeareDataset,\n",
    "    TokenMapping,\n",
    "    build_model,\n",
    "    next_token,\n",
    "    # Character-based helpers\n",
    "    encode_text,\n",
    "    # Subword-based helpers\n",
    "    encode_text_from_tokenizer,\n",
    "    tokenize_text_from_tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Deterministic training\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Attempt GPU; if not, stay on CPU\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters in text file: 9,771\n"
     ]
    }
   ],
   "source": [
    "# Reduced data to make it manageable for smaller systems\n",
    "DATA_FILE: str = '../data/shakespeare_small.txt'\n",
    "\n",
    "with open(DATA_FILE, 'r') as data_file:\n",
    "    raw_text = data_file.read()\n",
    "\n",
    "print(f'Number of characters in text file: {len(raw_text):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character-Based Text Generation\n",
    "\n",
    "The first model you'll build for text generation will use character-based\n",
    "tokens.\n",
    "\n",
    "Each token will be a single character from the text and the model will learn\n",
    "to predict the next character (a token).\n",
    "\n",
    "To generate text, the model will take in a new string,\n",
    "character-by-character, and then generate a new likely character based on the\n",
    "past input. Then the model will take into account that new character and\n",
    "generate the following character and so on and so on until the model has\n",
    "produced a set number of characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode Text into Integer Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def normalize_text(text: str) -> str:\n",
    "    # TODO: Normalize incoming text; can be multiple actions\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prince. where are the vile beginners of this fray?\n",
      "  ben. o noble prince. i can discover all\n",
      "    the unlucky manage of this fatal brawl.\n",
      "    there lies the man, slain by young romeo,\n",
      "    that slew thy kinsman, brave mercutio.\n",
      "  cap. wife. tybalt, my cousin! o my brother's child!\n",
      "    o prince! o husband! o, the blood is spill'd\n",
      "    of my dear kinsman! prince, as thou art true,\n",
      "    for blood of ours shed blood of montague.\n",
      "    o cousin, cousin!\n",
      "  prince. benvolio, who began this bloody fray?\n",
      "  ben\n"
     ]
    }
   ],
   "source": [
    "# TEST: Is your text normalized the way you expected?\n",
    "# Only the first 500 characters of the original text\n",
    "normalized_text = normalize_text(raw_text[:500])\n",
    "print(normalized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretokenize_text(text: str) -> str | list[str]:\n",
    "    # TODO: Pretokenize normalized text into character strings\n",
    "    smaller_pieces = [char for char in text]\n",
    "    return smaller_pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['p', 'r', 'i', 'n', 'c', 'e', '.', ' ', 'w', 'h', 'e', 'r', 'e', ' ', 'a', 'r', 'e', ' ', 't', 'h', 'e', ' ', 'v', 'i', 'l', 'e', ' ', 'b', 'e', 'g', 'i', 'n', 'n', 'e', 'r', 's', ' ', 'o', 'f', ' ', 't', 'h', 'i', 's', ' ', 'f', 'r', 'a', 'y', '?', '\\n', ' ', ' ', 'b', 'e', 'n', '.', ' ', 'o', ' ', 'n', 'o', 'b', 'l', 'e', ' ', 'p', 'r', 'i', 'n', 'c', 'e', '.', ' ', 'i', ' ', 'c', 'a', 'n', ' ', 'd', 'i', 's', 'c', 'o', 'v', 'e', 'r', ' ', 'a', 'l', 'l', '\\n', ' ', ' ', ' ', ' ', 't', 'h', 'e', ' ', 'u', 'n', 'l', 'u', 'c', 'k', 'y', ' ', 'm', 'a', 'n', 'a', 'g', 'e', ' ', 'o', 'f', ' ', 't', 'h', 'i', 's', ' ', 'f', 'a', 't', 'a', 'l', ' ', 'b', 'r', 'a', 'w', 'l', '.', '\\n', ' ', ' ', ' ', ' ', 't', 'h', 'e', 'r', 'e', ' ', 'l', 'i', 'e', 's', ' ', 't', 'h', 'e', ' ', 'm', 'a', 'n', ',', ' ', 's', 'l', 'a', 'i', 'n', ' ', 'b', 'y', ' ', 'y', 'o', 'u', 'n', 'g', ' ', 'r', 'o', 'm', 'e', 'o', ',', '\\n', ' ', ' ', ' ', ' ', 't', 'h', 'a', 't', ' ', 's', 'l', 'e', 'w', ' ', 't', 'h', 'y', ' ', 'k', 'i', 'n', 's', 'm', 'a', 'n', ',', ' ', 'b', 'r', 'a', 'v', 'e', ' ', 'm', 'e', 'r', 'c', 'u', 't', 'i', 'o', '.', '\\n', ' ', ' ', 'c', 'a', 'p', '.', ' ', 'w', 'i', 'f', 'e', '.', ' ', 't', 'y', 'b', 'a', 'l', 't', ',', ' ', 'm', 'y', ' ', 'c', 'o', 'u', 's', 'i', 'n', '!', ' ', 'o', ' ', 'm', 'y', ' ', 'b', 'r', 'o', 't', 'h', 'e', 'r', \"'\", 's', ' ', 'c', 'h', 'i', 'l', 'd', '!', '\\n', ' ', ' ', ' ', ' ', 'o', ' ', 'p', 'r', 'i', 'n', 'c', 'e', '!', ' ', 'o', ' ', 'h', 'u', 's', 'b', 'a', 'n', 'd', '!', ' ', 'o', ',', ' ', 't', 'h', 'e', ' ', 'b', 'l', 'o', 'o', 'd', ' ', 'i', 's', ' ', 's', 'p', 'i', 'l', 'l', \"'\", 'd', '\\n', ' ', ' ', ' ', ' ', 'o', 'f', ' ', 'm', 'y', ' ', 'd', 'e', 'a', 'r', ' ', 'k', 'i', 'n', 's', 'm', 'a', 'n', '!', ' ', 'p', 'r', 'i', 'n', 'c', 'e', ',', ' ', 'a', 's', ' ', 't', 'h', 'o', 'u', ' ', 'a', 'r', 't', ' ', 't', 'r', 'u', 'e', ',', '\\n', ' ', ' ', ' ', ' ', 'f', 'o', 'r', ' ', 'b', 'l', 'o', 'o', 'd', ' ', 'o', 'f', ' ', 'o', 'u', 'r', 's', ' ', 's', 'h', 'e', 'd', ' ', 'b', 'l', 'o', 'o', 'd', ' ', 'o', 'f', ' ', 'm', 'o', 'n', 't', 'a', 'g', 'u', 'e', '.', '\\n', ' ', ' ', ' ', ' ', 'o', ' ', 'c', 'o', 'u', 's', 'i', 'n', ',', ' ', 'c', 'o', 'u', 's', 'i', 'n', '!', '\\n', ' ', ' ', 'p', 'r', 'i', 'n', 'c', 'e', '.', ' ', 'b', 'e', 'n', 'v', 'o', 'l', 'i', 'o', ',', ' ', 'w', 'h', 'o', ' ', 'b', 'e', 'g', 'a', 'n', ' ', 't', 'h', 'i', 's', ' ', 'b', 'l', 'o', 'o', 'd', 'y', ' ', 'f', 'r', 'a', 'y', '?', '\\n', ' ', ' ', 'b', 'e', 'n']\n"
     ]
    }
   ],
   "source": [
    "# TEST: Is your (normalized) text pretokenized the way you expected?\n",
    "# Only the first 500 characters of the original text\n",
    "pretokenized_text = pretokenize_text(normalized_text)\n",
    "print(pretokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine normalization and pretokenization steps\n",
    "def tokenize_text(text: str) -> str | list[str]:\n",
    "    normalized_text: str = normalize_text(text)\n",
    "    pretokenized_text: str | list[str] = pretokenize_text(normalized_text)\n",
    "    # Characters are already tokens so pretokenized text is already tokenized\n",
    "    tokenized_text = pretokenized_text\n",
    "    return tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['p', 'r', 'i', 'n', 'c', 'e', '.', ' ', 'w', 'h', 'e', 'r', 'e', ' ', 'a', 'r', 'e', ' ', 't', 'h', 'e', ' ', 'v', 'i', 'l', 'e', ' ', 'b', 'e', 'g', 'i', 'n', 'n', 'e', 'r', 's', ' ', 'o', 'f', ' ', 't', 'h', 'i', 's', ' ', 'f', 'r', 'a', 'y', '?', '\\n', ' ', ' ', 'b', 'e', 'n', '.', ' ', 'o', ' ', 'n', 'o', 'b', 'l', 'e', ' ', 'p', 'r', 'i', 'n', 'c', 'e', '.', ' ', 'i', ' ', 'c', 'a', 'n', ' ', 'd', 'i', 's', 'c', 'o', 'v', 'e', 'r', ' ', 'a', 'l', 'l', '\\n', ' ', ' ', ' ', ' ', 't', 'h', 'e', ' ', 'u', 'n', 'l', 'u', 'c', 'k', 'y', ' ', 'm', 'a', 'n', 'a', 'g', 'e', ' ', 'o', 'f', ' ', 't', 'h', 'i', 's', ' ', 'f', 'a', 't', 'a', 'l', ' ', 'b', 'r', 'a', 'w', 'l', '.', '\\n', ' ', ' ', ' ', ' ', 't', 'h', 'e', 'r', 'e', ' ', 'l', 'i', 'e', 's', ' ', 't', 'h', 'e', ' ', 'm', 'a', 'n', ',', ' ', 's', 'l', 'a', 'i', 'n', ' ', 'b', 'y', ' ', 'y', 'o', 'u', 'n', 'g', ' ', 'r', 'o', 'm', 'e', 'o', ',', '\\n', ' ', ' ', ' ', ' ', 't', 'h', 'a', 't', ' ', 's', 'l', 'e', 'w', ' ', 't', 'h', 'y', ' ', 'k', 'i', 'n', 's', 'm', 'a', 'n', ',', ' ', 'b', 'r', 'a', 'v', 'e', ' ', 'm', 'e', 'r', 'c', 'u', 't', 'i', 'o', '.', '\\n', ' ', ' ', 'c', 'a', 'p', '.', ' ', 'w', 'i', 'f', 'e', '.', ' ', 't', 'y', 'b', 'a', 'l', 't', ',', ' ', 'm', 'y', ' ', 'c', 'o', 'u', 's', 'i', 'n', '!', ' ', 'o', ' ', 'm', 'y', ' ', 'b', 'r', 'o', 't', 'h', 'e', 'r', \"'\", 's', ' ', 'c', 'h', 'i', 'l', 'd', '!', '\\n', ' ', ' ', ' ', ' ', 'o', ' ', 'p', 'r', 'i', 'n', 'c', 'e', '!', ' ', 'o', ' ', 'h', 'u', 's', 'b', 'a', 'n', 'd', '!', ' ', 'o', ',', ' ', 't', 'h', 'e', ' ', 'b', 'l', 'o', 'o', 'd', ' ', 'i', 's', ' ', 's', 'p', 'i', 'l', 'l', \"'\", 'd', '\\n', ' ', ' ', ' ', ' ', 'o', 'f', ' ', 'm', 'y', ' ', 'd', 'e', 'a', 'r', ' ', 'k', 'i', 'n', 's', 'm', 'a', 'n', '!', ' ', 'p', 'r', 'i', 'n', 'c', 'e', ',', ' ', 'a', 's', ' ', 't', 'h', 'o', 'u', ' ', 'a', 'r', 't', ' ', 't', 'r', 'u', 'e', ',', '\\n', ' ', ' ', ' ', ' ', 'f', 'o', 'r', ' ', 'b', 'l', 'o', 'o', 'd', ' ', 'o', 'f', ' ', 'o', 'u', 'r', 's', ' ', 's', 'h', 'e', 'd', ' ', 'b', 'l', 'o', 'o', 'd', ' ', 'o', 'f', ' ', 'm', 'o', 'n', 't', 'a', 'g', 'u', 'e', '.', '\\n', ' ', ' ', ' ', ' ', 'o', ' ', 'c', 'o', 'u', 's', 'i', 'n', ',', ' ', 'c', 'o', 'u', 's', 'i', 'n', '!', '\\n', ' ', ' ', 'p', 'r', 'i', 'n', 'c', 'e', '.', ' ', 'b', 'e', 'n', 'v', 'o', 'l', 'i', 'o', ',', ' ', 'w', 'h', 'o', ' ', 'b', 'e', 'g', 'a', 'n', ' ', 't', 'h', 'i', 's', ' ', 'b', 'l', 'o', 'o', 'd', 'y', ' ', 'f', 'r', 'a', 'y', '?', '\\n', ' ', ' ', 'b', 'e', 'n']\n"
     ]
    }
   ],
   "source": [
    "# TEST: Is your tokenized text the way you expected?\n",
    "tokenized_text = tokenize_text(raw_text[:500])\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Postprocessing\n",
    "\n",
    "We'll skip postprocessing since we don't have any special tokens we want to\n",
    "consider for our task here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode (Tokens → Integer IDs)\n",
    "\n",
    "We have `encode_text()` from our helper module that can encode our text based on\n",
    "our tokenization process from our created `tokenize_text()` function.\n",
    "\n",
    "This will also provide us with `character_mapping`, an object that we can use to\n",
    "map our tokens back and forth from strings to integer IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_text, character_mapping = encode_text(raw_text, tokenize_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dataset: 9,772 characters\n"
     ]
    }
   ],
   "source": [
    "n_tokens = character_mapping.n_tokens\n",
    "dataset_size = len(encoded_text)\n",
    "print(f'Size of dataset: {dataset_size:,} characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining sequence length that will be taken in at a time by our model\n",
    "sequence_length = 32 # Number of characters\n",
    "batch_size = 32\n",
    "\n",
    "train_dataset = ShakespeareDataset(encoded_text, sequence_length)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=False, # Ensure deterministic training\n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model\n",
    "\n",
    "We'll provide a defined model today, but this could be a step that you would\n",
    "modify and experiment in other NLP projects you'll do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Defining the model to be trained and generate text with\n",
    "model = build_model(n_tokens)\n",
    "model.to(device)\n",
    "print(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Text Generation\n",
    "\n",
    "The `generate_text_by_char()` function will use your tokenizer and NLP model to\n",
    "generate new text token-by-token (character-by-character in this case) by taking\n",
    "in the input text and token sampling parameters.\n",
    "\n",
    "We can use temperature and top-k sampling to adjust the \"creativeness\" of the\n",
    "generated text.\n",
    "\n",
    "We also pass in the `num_chars` parameter to tell the function how many tokens\n",
    "(characters in this case) to generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_by_char(\n",
    "    input_str: str,\n",
    "    model,\n",
    "    token_mapping: TokenMapping = character_mapping,\n",
    "    num_chars: int = 100,\n",
    "    temperature: float = 1.0,\n",
    "    topk: int | None = None,\n",
    ") -> str:\n",
    "    # Uses your character-based tokenizer\n",
    "    tokenized_text: list[str] = tokenize_text(input_str)\n",
    "    # Generates token-by-token and creates a list of those tokens\n",
    "    generated_tokens = []\n",
    "    for _ in range(num_chars):\n",
    "        # Uses the input text and generated text (so far) to get next token\n",
    "        new_char = next_token(\n",
    "            tokenized_text=(tokenized_text + generated_tokens),\n",
    "            model=model,\n",
    "            token_mapping=token_mapping,\n",
    "            # Temperature & top-k sampling used in determining the next token\n",
    "            temperature=temperature,\n",
    "            topk=topk,\n",
    "            device=device,\n",
    "        )\n",
    "        generated_tokens.append(new_char)\n",
    "    # Returns input string plus the full generated string (of generated tokens)\n",
    "    full_text = ''.join(tokenized_text + generated_tokens)\n",
    "    return full_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "\n",
    "At this point, the model has not been trained so the code below will train the\n",
    "NLP model that will be used to generate new text.\n",
    "\n",
    "The model will take in the text data (broken by tokens by our character-based\n",
    "tokenizer) and attempt to predict the next token. Over time, the model should\n",
    "hopefully get better in predicting the next token (given the previous text).\n",
    "\n",
    "To help us visualize how the model is training, at the end of every epoch, we\n",
    "generate text using the `TEST_PHRASE` with the improving model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Loss: 2.9390148670946967\n",
      "[00m 1.6s (0 0.0) 2.7030]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to bed y  oc.o\n",
      "   i yorethi'oai iaha,  womwhthroa\n",
      "\n",
      "f  bie\n",
      " i\n",
      "t  ton whe. y mard\n",
      "-nec\n",
      "\n",
      "  thetgiatorm l.end\n",
      "Epoch 2/25, Loss: 2.5208502706934195\n",
      "[00m 2.9s (1 4.0) 2.4422]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to bes f    has measutherds,  ot orrase sillj\n",
      " erh a, beo slth!\n",
      "    ka\n",
      "   jtow ovse an d thant thael thed\n",
      "Epoch 3/25, Loss: 2.3508560430808148\n",
      "[00m 4.1s (2 8.0) 2.3152]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to bemd, h eek,\n",
      "      meut, pdeo sujhe.and\n",
      " y ind ootryhither m? spxthoth fis -for pasthe ee agj, nom yis\n",
      "Epoch 4/25, Loss: 2.2578898824629237\n",
      "[00m 5.3s (3 12.0) 2.2559]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be,  hand, what hor it wehat depod rod! youw\n",
      "    sil  me  fall luf pe ronln.\n",
      "    sghou of tbwor wou do\n",
      "Epoch 5/25, Loss: 2.189957589399619\n",
      "[00m 6.5s (4 16.0) 2.2088]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to beds\n",
      "    pa oty wange dne nerd,\n",
      ";\n",
      "     tbehis ofo mlaver,\n",
      "    t enceepur whalll, coed, youneus's-thaw \n",
      "Epoch 6/25, Loss: 2.133995934783435\n",
      "[00m 7.6s (5 20.0) 2.1602]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to bere is unt um knmalch.', \n",
      "    ind ftyband! thode, jomene na]tinugh!t haceerspet\n",
      "     as bulto eecet r\n",
      "Epoch 7/25, Loss: 2.0860666306292424\n",
      "[00m 8.7s (6 24.0) 2.1145]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to bede the damitriu])nje, bali. nucxat w ale deide, that pull thes tyfill whie will thet wwik tehe cope,\n",
      "Epoch 8/25, Loss: 2.0438105841152003\n",
      "[00m 9.8s (7 28.000000000000004) 2.0806]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to bere, brothalc?\n",
      "    ballo yhe ro, bave icde  wo dimer dim t'erinnle clorn ringoded\n",
      "  y my allt, tall b\n",
      "Epoch 9/25, Loss: 2.0067093528685023\n",
      "[00m 10.9s (8 32.0) 2.0545]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be vmating thanacepfidjorm!hort\n",
      "    todl iuk thet bout die'd wiend\n",
      "    rivel!\n",
      "   ill foul thais helt t\n",
      "Epoch 10/25, Loss: 1.9741355681028523\n",
      "[00m 12.0s (9 36.0) 2.0295]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to bet thals in somaerrt.   be ceesu iouch thanote su, wan foine ,o wou ofe t vovere tyball deper'd, i wi\n",
      "Epoch 11/25, Loss: 1.9454514225975412\n",
      "[00m 13.1s (10 40.0) 2.0052]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to ben in mew and thereat,\n",
      "    to priel ssdea, on deas; ib\n",
      "    have sole? all haden's bewenlo theetorse-w\n",
      "Epoch 12/25, Loss: 1.9199676783358464\n",
      "[00m 14.2s (11 44.0) 1.9816]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to being is tyby.\n",
      "    for.    rollled virlous'n chears brin te uat or gule yousito ay ishe. naue eeee.\n",
      "  \n",
      "Epoch 13/25, Loss: 1.8970361224940566\n",
      "[00m 15.3s (12 48.0) 1.9581]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to bet'!\n",
      " - eude bes someal. woeshe-blaol', all worse. heviall dead-hitj.\n",
      "    foul ilt tmreivind fresl, s\n",
      "Epoch 14/25, Loss: 1.8761529531635222\n",
      "[00m 16.5s (13 52.0) 1.9335]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to beave ber ulane pyan'rectyoud is hiy, dy hatithea? mhoom, wo romsy.\n",
      "  nurpov!\n",
      "    wo dew m? nlace, rom\n",
      "Epoch 15/25, Loss: 1.8571621531345806\n",
      "[00m 17.6s (14 56.00000000000001) 1.9084]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to beandeten\n",
      "    w,bold nis.\n",
      "    rivgite.\n",
      "    thou coud, shess\n",
      "    op, non so laill somer, brareese, bold\n",
      "Epoch 16/25, Loss: 1.8398858513988432\n",
      "[00m 18.7s (15 60.0) 1.8843]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to beod ;pand romeayes. i diack suld? haem, thou my if tond ene mere.\n",
      "    will he in cond son mow lolvin.\n",
      "Epoch 17/25, Loss: 1.8240132435423428\n",
      "[00m 19.8s (16 64.0) 1.8611]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be fose sours at, for b,ous sare some, wan, inght?\n",
      "  nursl,\n",
      "    to theding ai hepstle biaty\n",
      "    awh, t\n",
      "Epoch 18/25, Loss: 1.8093530496612924\n",
      "[00m 20.9s (17 68.0) 1.8379]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to beni.\n",
      "  nup bowen ches thye'd\n",
      "    thy wee bet romeo tyopuy divt fout dead.\n",
      ".    a tthere daker in ifen\n",
      "Epoch 19/25, Loss: 1.7956933054767672\n",
      "[00m 22.1s (18 72.0) 1.8161]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to bewin;\n",
      "    'iefmandigh; romee!    or ear winut that whethit youl'd\n",
      "    lese. wifen' he lid!.\n",
      "  jul mou\n",
      "Epoch 20/25, Loss: 1.7828627981123377\n",
      "[00m 23.3s (19 76.0) 1.7967]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to bet beifg;\n",
      "    bold, wol. hinnind,\n",
      "  s here!\n",
      "    iva'd adrighiwh gomert, nends.\n",
      "    ore, thet hain, an\n",
      "Epoch 21/25, Loss: 1.7706783548730318\n",
      "[00m 24.5s (20 80.0) 1.7789]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to bew is corgen. shithoginn enis, with whit,\n",
      "  'racllousither jule is-llighesw thoard ay winjee!\n",
      "    tar\n",
      "Epoch 22/25, Loss: 1.7590531374587388\n",
      "[00m 25.7s (21 84.0) 1.7617]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to best wor goathcord haent gubarm, ming wial,\n",
      "    whot come, wil there wordr wolloneoaths\n",
      "    thert. no \n",
      "Epoch 23/25, Loss: 1.7479679609908432\n",
      "[00m 26.8s (22 88.0) 1.7454]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to bes fort normory tusur'd\n",
      "    anves dair wheack. o fay maing learch that kolls' to blouse bead,\n",
      "    wha\n",
      "Epoch 24/25, Loss: 1.7373703378145813\n",
      "[00m 27.9s (23 92.0) 1.7302]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be, \n",
      "    will, wo leolse to to fedsite. to his male.\n",
      "    i sill bites ponthay deamt rithoursince.\n",
      "  nu\n",
      "Epoch 25/25, Loss: 1.7271897026749907\n",
      "[00m 29.0s (24 96.0) 1.7158]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to bead, s und rowe.\n",
      "    taly thive hadir where he to con few'er woolr but bw when my deam, romeo!\n",
      "    of\n"
     ]
    }
   ],
   "source": [
    "TEST_PHRASE = 'To be or not to be'\n",
    "# Use more epochs if not CPU device\n",
    "epochs = 5 if device == 'cpu' else 25\n",
    "\n",
    "start = start_time()\n",
    "for epoch in range(epochs):\n",
    "    # Set model into \"training mode\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_batch.to(device))\n",
    "        loss = criterion(output.transpose(1, 2), y_batch.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f'Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(train_loader)}')\n",
    "    print(f'[{time_since(start)} ({epoch} {epoch / epochs * 100}) {loss:.4f}]')\n",
    "    print('-'*72)\n",
    "    gen_output = generate_text_by_char(\n",
    "        input_str=TEST_PHRASE,\n",
    "        model=model,\n",
    "        num_chars=100,\n",
    "    )\n",
    "    print(gen_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Text\n",
    "\n",
    "Now that the model has been trained, go ahead and observe how it performs!\n",
    "\n",
    "Try adjusting the different sampling methods using the `temperature` and `topk`\n",
    "parameters on the same input string to see the differences.\n",
    "\n",
    "You might also try different phrases as well as how many tokens (`num_chars`) to\n",
    "generate and observe how it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to be or not to bere. wake o hnant hath mayl fot mof! nemiod, wit, maye bet weak thes beath teach with loven\n",
      "    blood\n"
     ]
    }
   ],
   "source": [
    "output = generate_text_by_char(\n",
    "    input_str='To be or not to be',\n",
    "    model=model,\n",
    "    num_chars=100,\n",
    "    temperature=1.0,\n",
    "    topk=None,\n",
    ")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subword Text Generation\n",
    "\n",
    "The next model you'll build will use subword-tokenization instead of \n",
    "characters-based token to train a model and ultimately generate new text\n",
    "token-by-token.\n",
    "\n",
    "Although this could be done by creating your own tokenizer, you'll use\n",
    "Hugging Face to use a pretrained tokenizer to tokenize the data.\n",
    "\n",
    "After training the model with subword tokens, \n",
    "the model will take in a new string, token-by-token, and then generate a new\n",
    "token (subword).\n",
    "The model will continue producing new subword tokens based on the input text\n",
    "and already produced tokens until a set number of tokens have been generated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode Text into Integer Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing a Tokenizer\n",
    "\n",
    "> NOTE:\n",
    "> \n",
    "> You can load another model outside of these choices but the model\n",
    "> will have to be downloaded and may or may not be effective.\n",
    ">\n",
    "> If you'd like to explore more, here's a link to you might want to start with\n",
    "> of different available pretrained models on Hugging Face:\n",
    "> https://huggingface.co/models?pipeline_tag=text-generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Choose a pretrained tokenizer to use:\n",
    "\n",
    "# Docs: https://huggingface.co/xlm-roberta-base\n",
    "# model_name = 'xlm-roberta-base'\n",
    "# DOCS: https://huggingface.co/bert-base-cased\n",
    "# model_name = 'bert-base-cased'\n",
    "# DOCS: https://huggingface.co/bert-base-uncased \n",
    "# model_name = 'bert-base-uncased'\n",
    "model_name = 'bert-base-uncased'\n",
    "my_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode (Tokens → Integer IDs)\n",
    "\n",
    "We have `encode_text_from_tokenizer()` from our helper module that can encode\n",
    "our text based on our tokenization process from our tokenizer `my_tokenizer`.\n",
    "\n",
    "This will also provide us with `token_mapping`, an object that we can use to\n",
    "map our tokens back and forth from strings to integer IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_text, token_mapping = encode_text_from_tokenizer(\n",
    "    text=raw_text,\n",
    "    tokenizer=my_tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dataset: 2,456 tokens\n"
     ]
    }
   ],
   "source": [
    "n_tokens = token_mapping.n_tokens\n",
    "dataset_size = len(encoded_text)\n",
    "print(f'Size of dataset: {dataset_size:,} tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining sequence length that will be taken in at a time by our model\n",
    "sequence_length = 32 # Number of tokens\n",
    "batch_size = 32\n",
    "\n",
    "train_dataset = ShakespeareDataset(encoded_text, sequence_length)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=False, # Ensure deterministic training\n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model\n",
    "\n",
    "We'll provide a defined model today, but this could be a step that you would\n",
    "modify and experiment in other NLP projects you'll do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the model to be trained and generate text with\n",
    "model = build_model(n_tokens)\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Text Generation\n",
    "\n",
    "The `generate_text_by_subword()` function will use your chosen tokenizer and the\n",
    "NLP model to generate new text token-by-token (subwords in this case) by taking\n",
    "in the input text and token sampling parameters.\n",
    "\n",
    "We can use temperature and top-k sampling to adjust the \"creativeness\" of the\n",
    "generated text.\n",
    "\n",
    "We also pass in the `num_tokens` parameter to tell the function how many\n",
    "(subword)tokens to generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_by_subword(\n",
    "    input_str: str,\n",
    "    model,\n",
    "    token_mapping: TokenMapping = token_mapping,\n",
    "    tokenizer = my_tokenizer,\n",
    "    num_tokens: int = 100,\n",
    "    temperature: float = 1.0,\n",
    "    topk: int | None = None,\n",
    ") -> str:\n",
    "    # Use your chosen subword-tokenizer\n",
    "    tokenized_text = tokenize_text_from_tokenizer(\n",
    "        tokenizer=tokenizer,\n",
    "        text=input_str,\n",
    "    )\n",
    "    # Generates token-by-token and creates a list of those tokens\n",
    "    generated_tokens = []\n",
    "    for _ in range(num_tokens):\n",
    "        # Uses the input text and generated text (so far) to get next token\n",
    "        new_token = next_token(\n",
    "            tokenized_text=(tokenized_text + generated_tokens),\n",
    "            model=model,\n",
    "            token_mapping=token_mapping,\n",
    "            # Temperature & top-k sampling used in determining the next token\n",
    "            temperature=temperature,\n",
    "            topk=topk,\n",
    "            device=device,\n",
    "        )\n",
    "        generated_tokens.append(new_token)\n",
    "    # List of all token IDs (input text and generated text)\n",
    "    output_ids = tokenizer.convert_tokens_to_ids(\n",
    "        tokenized_text + generated_tokens\n",
    "    )\n",
    "    # Returns input string plus the full generated string from list of token IDs\n",
    "    full_text = tokenizer.decode(output_ids)\n",
    "    return full_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "\n",
    "At this point, the model has not been trained so the code below will train the\n",
    "NLP model that will be used to generate new text.\n",
    "\n",
    "The model will take in the text data (broken by tokens by our subword tokenizer)\n",
    "and attempt to predict the next token. Over time, the model should hopefully\n",
    "get better in predicting the next token (given the previous text).\n",
    "\n",
    "To help us visualize how the model is training, at the end of every epoch, we\n",
    "generate text using the `TEST_PHRASE` with the improving model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Loss: 6.491715744922035\n",
      "[00m 0.4s (0 0.0) 6.2052]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be leap fetchght? bringsoe motion than god ou rude a nice meishmentat new must dearviousuil following simple how slain?ink general pit them\n",
      "Epoch 2/25, Loss: 5.796372150119982\n",
      "[00m 0.7s (1 4.0) 5.4181]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be? comes all he, that say'no down wink fai to he, we'' that i, fiery'brave that. nurse there but '\n",
      "Epoch 3/25, Loss: 5.537153582823904\n",
      "[00m 0.9s (2 8.0) 5.3079]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be you - the! whom friends d i word cords jul my oforn true wagon i strong. followed, snowbal are spread s i sober ty one\n",
      "Epoch 4/25, Loss: 5.443774323714407\n",
      "[00m 1.2s (3 12.0) 5.2120]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be ; in! husband take justice but so forget, hold well part bloody it s,rse tyrant not night ben speakmann, do come we, '\n",
      "Epoch 5/25, Loss: 5.3503396636561344\n",
      "[00m 1.5s (4 16.0) 5.1217]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be all. poor if ; come it prince my but my art ty foolish guilty villain wo he the banished laurence's such, cold s'that '\n",
      "Epoch 6/25, Loss: 5.252743626895704\n",
      "[00m 1.9s (5 20.0) 5.0325]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be or in, gentle sbal art no dost those gentle s,,t name to. romeoew it martial, to bloodylvis i for bow\n",
      "Epoch 7/25, Loss: 5.151670336723328\n",
      "[00m 2.3s (6 24.0) 4.9397]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be of comfort then sl prince my'sbal by shut,. ted wound!, beg brow - ever whose weeping enterbalg, that his '\n",
      "Epoch 8/25, Loss: 5.049852659827785\n",
      "[00m 2.6s (7 28.000000000000004) 4.8471]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be. robes, them! cries!. i in sm abuses pleading he co love. romeo will resign need proceeding jul, - mark red. is\n",
      "Epoch 9/25, Loss: 4.947451779716893\n",
      "[00m 2.9s (8 32.0) 4.7589]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be, and death? ted you blows - my that is new, myh enter it d,ink! was in shall, when his father cousin thy\n",
      "Epoch 10/25, Loss: 4.844675249175022\n",
      "[00m 3.1s (9 36.0) 4.6768]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be, the ; trust? jul brows all him.! ah o'sh or. is their brow will eyes ; come, of that part\n",
      "Epoch 11/25, Loss: 4.741901065173902\n",
      "[00m 3.5s (10 40.0) 4.6008]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be for back, slain dead back bid stay find,'s piercing living some. was's brave romeo with may to. o, there, and\n",
      "Epoch 12/25, Loss: 4.639633122243379\n",
      "[00m 3.9s (11 44.0) 4.5298]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be dead his back, in fellowshiph give. lives night s these.! these.r ; ever me - night bring.leen mark. our,\n",
      "Epoch 13/25, Loss: 4.538205369522697\n",
      "[00m 4.2s (12 48.0) 4.4618]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be - not did, to'true dead, agileous's dead, all, what friend.'s my ty slain ala to strifecut may\n",
      "Epoch 14/25, Loss: 4.438044595090966\n",
      "[00m 4.6s (13 52.0) 4.3950]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be draw to blood your cannot gentle revenge's i blood, and cries woe. imannst the hell death modest whip, it dove i live\n",
      "Epoch 15/25, Loss: 4.339692451451954\n",
      "[00m 4.9s (14 56.00000000000001) 4.3294]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be as in him false, are a and in palace is crown prayers wife, blood prince the theeend in thatlio to, his abuses honour!e\n",
      "Epoch 16/25, Loss: 4.243381606905084\n",
      "[00m 5.2s (15 60.0) 4.2644]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be it.!l andity your a gorgeous, vita twentyron here fetch nurses will withal your close honest turn eye re rank with damned trust\n",
      "Epoch 17/25, Loss: 4.149212263132396\n",
      "[00m 5.4s (16 64.0) 4.1991]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be?s, worse that simplecut lies canous bow it?, and do are did? the bi noh, ( of lai'thybal\n",
      "Epoch 18/25, Loss: 4.05724351970773\n",
      "[00m 5.8s (17 68.0) 4.1330]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be'd romeo you though i day, and'that i once lo my lo where eyes,s sober ex nice your bleeding mark the here me then\n",
      "Epoch 19/25, Loss: 3.96766060590744\n",
      "[00m 6.1s (18 72.0) 4.0662]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be hesw my per thou but of, did owee. iba! twenty this, thyew those i to the lodging. o, heaven ty\n",
      "Epoch 20/25, Loss: 3.8805529506582963\n",
      "[00m 6.4s (19 76.0) 3.9993]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be ; what palace hegled did!io!? ; come, lies's there ;. give all is living, kill romeo, a -vo\n",
      "Epoch 21/25, Loss: 3.7959060292494926\n",
      "[00m 6.6s (20 80.0) 3.9328]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be. jul. wo what tocutio think had hence thisew this,h o, with romeo night made bank d, whichg 'eous it\n",
      "Epoch 22/25, Loss: 3.713701910094211\n",
      "[00m 6.9s (21 84.0) 3.8672]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be herence. have, thou. and andend! hold fled did! learn that world followed him,h! o in heavy o, which [\n",
      "Epoch 23/25, Loss: 3.6338817257630196\n",
      "[00m 7.2s (22 88.0) 3.8025]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be of't hum tears romeoat'fault answer now but'sorn's death, which determine dwell dexterrk slain, if damned!io\n",
      "Epoch 24/25, Loss: 3.556359771050905\n",
      "[00m 7.6s (23 92.0) 3.7389]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be -bal allous to weing ben littlelio. friends, mother, mother the law guilty face of out said dreadfulace that life and what has\n",
      "Epoch 25/25, Loss: 3.4810515830391333\n",
      "[00m 8.2s (24 96.0) 3.6764]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be worse this on e once more thou to romeo of heavy blood, he'd my dear there jul am! vile just e once - the day rites\n"
     ]
    }
   ],
   "source": [
    "TEST_PHRASE = 'To be or not to be'\n",
    "# Use more epochs if not CPU device\n",
    "epochs = 5 if device == 'cpu' else 25\n",
    "\n",
    "start = start_time()\n",
    "for epoch in range(epochs):\n",
    "    # Set model into \"training mode\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_batch.to(device))\n",
    "        loss = criterion(output.transpose(1, 2), y_batch.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f'Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(train_loader)}')\n",
    "    print(f'[{time_since(start)} ({epoch} {epoch / epochs * 100}) {loss:.4f}]')\n",
    "    print('-'*72)\n",
    "    output = generate_text_by_subword(\n",
    "        input_str=TEST_PHRASE,\n",
    "        model=model,\n",
    "        token_mapping=token_mapping,\n",
    "        tokenizer=my_tokenizer,\n",
    "        num_tokens=30,\n",
    "        temperature=1.0,\n",
    "    )\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Text\n",
    "\n",
    "Now that the model has been trained, go ahead and observe how it performs!\n",
    "\n",
    "Try adjusting the different sampling methods using the `temperature` and `topk`\n",
    "parameters on the same input string to see the differences.\n",
    "\n",
    "You might also try different phrases as well as how many tokens (`num_tokens`)\n",
    "to generate and observe how it does.\n",
    "\n",
    "------------\n",
    "\n",
    "Consider how this model differs from the results from the text generation using\n",
    "the character-based tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to be or not to be thy three such, husband who. galloew so'tybalace!, find! nurse., thy ; and ; underneath cousin, loving!\n"
     ]
    }
   ],
   "source": [
    "output = generate_text_by_subword(\n",
    "        input_str='To be or not to be',\n",
    "        model=model,\n",
    "        token_mapping=token_mapping,\n",
    "        tokenizer=my_tokenizer,\n",
    "        num_tokens=30,\n",
    "        temperature=1.5,\n",
    "        topk=100,\n",
    "    )\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
